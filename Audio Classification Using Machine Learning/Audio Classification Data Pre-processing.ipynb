{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a384ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_file_path=r\"UrbanSound8K/UrbanSound8K/audio_file.wav\"\n",
    "wave_data,wave_sampling_rate=librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f9abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3300ab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00559841, 0.00670769, 0.00185249, ..., 0.00281176, 0.00390908,\n",
       "       0.00646727], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf75e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wave_data_float=wave_data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b451537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aa8116e230>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(wave_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7433a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "wave_sampling_rate_1,wave_data_1=wav.read(audio_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7158e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_sampling_rate_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d530931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([251, 252, 216, ..., 183, 202, 212], dtype=int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae9544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aaff45b5b0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(wave_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aeef4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 55)\n"
     ]
    }
   ],
   "source": [
    "mfccs=librosa.feature.mfcc(y=wave_data_float,sr=wave_sampling_rate,n_mfcc=40)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d647c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.26161163e+02, -2.87269470e+02, -2.88447083e+02, ...,\n",
       "        -2.81561829e+02, -2.96657257e+02, -3.31516693e+02],\n",
       "       [ 1.21410469e+02,  1.27863358e+02,  1.22758675e+02, ...,\n",
       "         1.14523148e+02,  1.18969223e+02,  1.13537567e+02],\n",
       "       [ 2.57392197e+01,  3.24453888e+01,  4.15492859e+01, ...,\n",
       "         3.07786674e+01,  3.38617935e+01,  3.83849945e+01],\n",
       "       ...,\n",
       "       [-3.82446980e+00, -3.90522742e+00, -6.04407310e+00, ...,\n",
       "         2.25742102e+00,  1.05161023e+00, -5.08559799e+00],\n",
       "       [ 2.15916872e+00, -1.70459962e+00,  1.71750903e-01, ...,\n",
       "        -1.92350316e+00, -2.27748871e-01, -3.15721035e+00],\n",
       "       [-4.08110523e+00, -4.49795818e+00, -2.74705410e+00, ...,\n",
       "        -8.10234070e+00, -8.60533714e+00, -6.65715885e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77824b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "audio_dataset_path=r'UrbanSound8K/UrbanSound8K/audio/'\n",
    "metadata=pd.read_csv('UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d7fb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "820755a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resampy\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "                                              0.0/3.1 MB ? eta -:--:--\n",
      "     -                                        0.1/3.1 MB 4.3 MB/s eta 0:00:01\n",
      "     -----                                    0.4/3.1 MB 5.1 MB/s eta 0:00:01\n",
      "     -------                                  0.6/3.1 MB 4.8 MB/s eta 0:00:01\n",
      "     -----------                              0.9/3.1 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------                           1.1/3.1 MB 5.0 MB/s eta 0:00:01\n",
      "     -----------------                        1.4/3.1 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------------                     1.6/3.1 MB 5.1 MB/s eta 0:00:01\n",
      "     -----------------------                  1.8/3.1 MB 5.3 MB/s eta 0:00:01\n",
      "     --------------------------               2.0/3.1 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------             2.2/3.1 MB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------          2.4/3.1 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------------       2.7/3.1 MB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------    2.9/3.1 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.1/3.1 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aneesh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from resampy) (1.23.5)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\aneesh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from resampy) (0.57.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\aneesh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba>=0.53->resampy) (0.40.1rc1)\n",
      "Installing collected packages: resampy\n",
      "Successfully installed resampy-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2f9d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: resampy\n",
      "Version: 0.4.2\n",
      "Summary: Efficient signal resampling\n",
      "Home-page: https://github.com/bmcfee/resampy\n",
      "Author: Brian McFee\n",
      "Author-email: brian.mcfee@nyu.edu\n",
      "License: ISC\n",
      "Location: c:\\users\\aneesh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: numba, numpy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16fdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3555it [04:25, 17.63it/s]C:\\Users\\Aneesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8324it [09:13, 23.91it/s]C:\\Users\\Aneesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aneesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [09:36, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import resampy\n",
    "\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features = []\n",
    "for index_num, row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path), 'fold' + str(row[\"fold\"]) + '/', str(row[\"slice_file_name\"]))\n",
    "    final_class_labels = row[\"class\"]\n",
    "    data = features_extractor(file_name)\n",
    "    extracted_features.append([data, final_class_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee21088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-217.35526, 70.22338, -130.38527, -53.282898,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.09818, 109.34077, -52.919525, 60.86475, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-458.79114, 121.38419, -46.520657, 52.00812, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-413.89984, 101.66373, -35.42945, 53.036358, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-446.60352, 113.68541, -52.402206, 60.302044,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-217.35526, 70.22338, -130.38527, -53.282898,...          dog_bark\n",
       "1  [-424.09818, 109.34077, -52.919525, 60.86475, ...  children_playing\n",
       "2  [-458.79114, 121.38419, -46.520657, 52.00812, ...  children_playing\n",
       "3  [-413.89984, 101.66373, -35.42945, 53.036358, ...  children_playing\n",
       "4  [-446.60352, 113.68541, -52.402206, 60.302044,...  children_playing"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057dcd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c903ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7918e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "y=np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f26f13a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45980449",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba15a188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.31104706e+02,  1.12505905e+02, -2.25746956e+01, ...,\n",
       "         3.24665213e+00, -1.36902380e+00,  2.75575471e+00],\n",
       "       [-1.36703424e+01,  9.10850830e+01, -7.79273319e+00, ...,\n",
       "        -3.25305080e+00, -5.27745247e+00, -1.55697179e+00],\n",
       "       [-4.98715439e+01,  2.65352994e-01, -2.05009365e+01, ...,\n",
       "         2.85459447e+00, -1.60920465e+00,  3.52480578e+00],\n",
       "       ...,\n",
       "       [-4.27012360e+02,  9.26230469e+01,  3.12939739e+00, ...,\n",
       "         7.42641389e-01,  7.33490944e-01,  7.11009085e-01],\n",
       "       [-1.45754608e+02,  1.36265778e+02, -3.35155182e+01, ...,\n",
       "         1.46811950e+00, -2.00917006e+00, -8.82181883e-01],\n",
       "       [-4.21031342e+02,  2.10654541e+02,  3.49066067e+00, ...,\n",
       "        -5.38886738e+00, -3.37136054e+00, -1.56651151e+00]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e7516fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b79e2038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6985, 40)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d45b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 40)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f12d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6985, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e74f62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b366109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b91d7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46bb6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb66214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7e4f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               4100      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,410\n",
      "Trainable params: 45,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c61cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56dd5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0408 - accuracy: 0.7196\n",
      "Epoch 1: val_loss improved from inf to 1.08763, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0377 - accuracy: 0.7201 - val_loss: 1.0876 - val_accuracy: 0.7430\n",
      "Epoch 2/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0067 - accuracy: 0.7145\n",
      "Epoch 2: val_loss did not improve from 1.08763\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0017 - accuracy: 0.7154 - val_loss: 1.0961 - val_accuracy: 0.7424\n",
      "Epoch 3/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0679 - accuracy: 0.7148\n",
      "Epoch 3: val_loss improved from 1.08763 to 1.07950, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0579 - accuracy: 0.7168 - val_loss: 1.0795 - val_accuracy: 0.7407\n",
      "Epoch 4/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0078 - accuracy: 0.7137\n",
      "Epoch 4: val_loss did not improve from 1.07950\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0077 - accuracy: 0.7132 - val_loss: 1.1197 - val_accuracy: 0.7407\n",
      "Epoch 5/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0071 - accuracy: 0.7163\n",
      "Epoch 5: val_loss did not improve from 1.07950\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0046 - accuracy: 0.7168 - val_loss: 1.1242 - val_accuracy: 0.7453\n",
      "Epoch 6/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.9802 - accuracy: 0.7213\n",
      "Epoch 6: val_loss did not improve from 1.07950\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9941 - accuracy: 0.7204 - val_loss: 1.0903 - val_accuracy: 0.7453\n",
      "Epoch 7/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9840 - accuracy: 0.7169\n",
      "Epoch 7: val_loss improved from 1.07950 to 1.07111, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9861 - accuracy: 0.7162 - val_loss: 1.0711 - val_accuracy: 0.7539\n",
      "Epoch 8/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0060 - accuracy: 0.7221\n",
      "Epoch 8: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.0063 - accuracy: 0.7224 - val_loss: 1.1023 - val_accuracy: 0.7527\n",
      "Epoch 9/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9891 - accuracy: 0.7200\n",
      "Epoch 9: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9891 - accuracy: 0.7198 - val_loss: 1.1442 - val_accuracy: 0.7499\n",
      "Epoch 10/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0184 - accuracy: 0.7116\n",
      "Epoch 10: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0281 - accuracy: 0.7112 - val_loss: 1.1033 - val_accuracy: 0.7378\n",
      "Epoch 11/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0420 - accuracy: 0.7085\n",
      "Epoch 11: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0519 - accuracy: 0.7089 - val_loss: 1.1092 - val_accuracy: 0.7556\n",
      "Epoch 12/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.9992 - accuracy: 0.7186\n",
      "Epoch 12: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0166 - accuracy: 0.7167 - val_loss: 1.1447 - val_accuracy: 0.7579\n",
      "Epoch 13/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0261 - accuracy: 0.7118\n",
      "Epoch 13: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0322 - accuracy: 0.7130 - val_loss: 1.1274 - val_accuracy: 0.7476\n",
      "Epoch 14/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.9876 - accuracy: 0.7148\n",
      "Epoch 14: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9926 - accuracy: 0.7161 - val_loss: 1.1183 - val_accuracy: 0.7579\n",
      "Epoch 15/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0324 - accuracy: 0.7082\n",
      "Epoch 15: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0439 - accuracy: 0.7101 - val_loss: 1.1043 - val_accuracy: 0.7453\n",
      "Epoch 16/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.9820 - accuracy: 0.7153\n",
      "Epoch 16: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 0.9820 - accuracy: 0.7157 - val_loss: 1.1104 - val_accuracy: 0.7596\n",
      "Epoch 17/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0125 - accuracy: 0.7110\n",
      "Epoch 17: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0208 - accuracy: 0.7097 - val_loss: 1.1050 - val_accuracy: 0.7504\n",
      "Epoch 18/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0218 - accuracy: 0.7145\n",
      "Epoch 18: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0331 - accuracy: 0.7117 - val_loss: 1.1442 - val_accuracy: 0.7401\n",
      "Epoch 19/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0104 - accuracy: 0.7291\n",
      "Epoch 19: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0071 - accuracy: 0.7286 - val_loss: 1.0775 - val_accuracy: 0.7602\n",
      "Epoch 20/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0522 - accuracy: 0.7154\n",
      "Epoch 20: val_loss did not improve from 1.07111\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0526 - accuracy: 0.7141 - val_loss: 1.0856 - val_accuracy: 0.7527\n",
      "Epoch 21/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0168 - accuracy: 0.7175\n",
      "Epoch 21: val_loss improved from 1.07111 to 1.04393, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.0031 - accuracy: 0.7170 - val_loss: 1.0439 - val_accuracy: 0.7556\n",
      "Epoch 22/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0582 - accuracy: 0.7168\n",
      "Epoch 22: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0500 - accuracy: 0.7165 - val_loss: 1.1178 - val_accuracy: 0.7567\n",
      "Epoch 23/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0946 - accuracy: 0.7087\n",
      "Epoch 23: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0923 - accuracy: 0.7074 - val_loss: 1.0625 - val_accuracy: 0.7550\n",
      "Epoch 24/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0787 - accuracy: 0.7068\n",
      "Epoch 24: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0689 - accuracy: 0.7069 - val_loss: 1.0544 - val_accuracy: 0.7459\n",
      "Epoch 25/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.7159\n",
      "Epoch 25: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0347 - accuracy: 0.7161 - val_loss: 1.0671 - val_accuracy: 0.7596\n",
      "Epoch 26/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0491 - accuracy: 0.6991\n",
      "Epoch 26: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0394 - accuracy: 0.7009 - val_loss: 1.0761 - val_accuracy: 0.7493\n",
      "Epoch 27/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9772 - accuracy: 0.7151\n",
      "Epoch 27: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9778 - accuracy: 0.7161 - val_loss: 1.0979 - val_accuracy: 0.7464\n",
      "Epoch 28/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0422 - accuracy: 0.7086\n",
      "Epoch 28: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0404 - accuracy: 0.7078 - val_loss: 1.0470 - val_accuracy: 0.7550\n",
      "Epoch 29/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0651 - accuracy: 0.7202\n",
      "Epoch 29: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0666 - accuracy: 0.7214 - val_loss: 1.0747 - val_accuracy: 0.7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0220 - accuracy: 0.7178\n",
      "Epoch 30: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0220 - accuracy: 0.7178 - val_loss: 1.0818 - val_accuracy: 0.7487\n",
      "Epoch 31/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.7167\n",
      "Epoch 31: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0348 - accuracy: 0.7164 - val_loss: 1.0722 - val_accuracy: 0.7567\n",
      "Epoch 32/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.7147\n",
      "Epoch 32: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0281 - accuracy: 0.7151 - val_loss: 1.0704 - val_accuracy: 0.7550\n",
      "Epoch 33/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9987 - accuracy: 0.7140\n",
      "Epoch 33: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9992 - accuracy: 0.7151 - val_loss: 1.1243 - val_accuracy: 0.7562\n",
      "Epoch 34/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.9708 - accuracy: 0.7232\n",
      "Epoch 34: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9688 - accuracy: 0.7233 - val_loss: 1.1382 - val_accuracy: 0.7321\n",
      "Epoch 35/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.7144\n",
      "Epoch 35: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0423 - accuracy: 0.7142 - val_loss: 1.1184 - val_accuracy: 0.7504\n",
      "Epoch 36/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0881 - accuracy: 0.7112\n",
      "Epoch 36: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0881 - accuracy: 0.7110 - val_loss: 1.0582 - val_accuracy: 0.7533\n",
      "Epoch 37/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0670 - accuracy: 0.7144\n",
      "Epoch 37: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0680 - accuracy: 0.7127 - val_loss: 1.1263 - val_accuracy: 0.7550\n",
      "Epoch 38/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0725 - accuracy: 0.7148\n",
      "Epoch 38: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0716 - accuracy: 0.7144 - val_loss: 1.0648 - val_accuracy: 0.7533\n",
      "Epoch 39/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0079 - accuracy: 0.7140\n",
      "Epoch 39: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0076 - accuracy: 0.7138 - val_loss: 1.0741 - val_accuracy: 0.7430\n",
      "Epoch 40/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0375 - accuracy: 0.7179\n",
      "Epoch 40: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0551 - accuracy: 0.7167 - val_loss: 1.0729 - val_accuracy: 0.7464\n",
      "Epoch 41/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0289 - accuracy: 0.7070\n",
      "Epoch 41: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0199 - accuracy: 0.7082 - val_loss: 1.1227 - val_accuracy: 0.7487\n",
      "Epoch 42/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0754 - accuracy: 0.7063\n",
      "Epoch 42: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0737 - accuracy: 0.7064 - val_loss: 1.0636 - val_accuracy: 0.7504\n",
      "Epoch 43/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0137 - accuracy: 0.7167\n",
      "Epoch 43: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0125 - accuracy: 0.7158 - val_loss: 1.0609 - val_accuracy: 0.7470\n",
      "Epoch 44/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0725 - accuracy: 0.7149\n",
      "Epoch 44: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0688 - accuracy: 0.7150 - val_loss: 1.0611 - val_accuracy: 0.7504\n",
      "Epoch 45/375\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 1.0351 - accuracy: 0.7184\n",
      "Epoch 45: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0330 - accuracy: 0.7160 - val_loss: 1.0947 - val_accuracy: 0.7493\n",
      "Epoch 46/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.1571 - accuracy: 0.7159\n",
      "Epoch 46: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1473 - accuracy: 0.7160 - val_loss: 1.0602 - val_accuracy: 0.7653\n",
      "Epoch 47/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.1271 - accuracy: 0.7121\n",
      "Epoch 47: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.1236 - accuracy: 0.7099 - val_loss: 1.0839 - val_accuracy: 0.7510\n",
      "Epoch 48/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0468 - accuracy: 0.7101\n",
      "Epoch 48: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0450 - accuracy: 0.7105 - val_loss: 1.0705 - val_accuracy: 0.7493\n",
      "Epoch 49/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0434 - accuracy: 0.7031\n",
      "Epoch 49: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0434 - accuracy: 0.7031 - val_loss: 1.0723 - val_accuracy: 0.7493\n",
      "Epoch 50/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0023 - accuracy: 0.7202\n",
      "Epoch 50: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9998 - accuracy: 0.7211 - val_loss: 1.1073 - val_accuracy: 0.7567\n",
      "Epoch 51/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0565 - accuracy: 0.7109\n",
      "Epoch 51: val_loss did not improve from 1.04393\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0572 - accuracy: 0.7118 - val_loss: 1.0985 - val_accuracy: 0.7487\n",
      "Epoch 52/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0493 - accuracy: 0.7054\n",
      "Epoch 52: val_loss improved from 1.04393 to 1.03548, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0344 - accuracy: 0.7074 - val_loss: 1.0355 - val_accuracy: 0.7619\n",
      "Epoch 53/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9659 - accuracy: 0.7193\n",
      "Epoch 53: val_loss did not improve from 1.03548\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9718 - accuracy: 0.7168 - val_loss: 1.0615 - val_accuracy: 0.7504\n",
      "Epoch 54/375\n",
      " 93/110 [========================>.....] - ETA: 0s - loss: 1.0314 - accuracy: 0.7127\n",
      "Epoch 54: val_loss did not improve from 1.03548\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0463 - accuracy: 0.7101 - val_loss: 1.0673 - val_accuracy: 0.7556\n",
      "Epoch 55/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9992 - accuracy: 0.7138\n",
      "Epoch 55: val_loss did not improve from 1.03548\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9999 - accuracy: 0.7142 - val_loss: 1.0812 - val_accuracy: 0.7550\n",
      "Epoch 56/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0575 - accuracy: 0.7096\n",
      "Epoch 56: val_loss did not improve from 1.03548\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0539 - accuracy: 0.7107 - val_loss: 1.1227 - val_accuracy: 0.7447\n",
      "Epoch 57/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0063 - accuracy: 0.7172\n",
      "Epoch 57: val_loss did not improve from 1.03548\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0020 - accuracy: 0.7155 - val_loss: 1.0742 - val_accuracy: 0.7556\n",
      "Epoch 58/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0663 - accuracy: 0.7159\n",
      "Epoch 58: val_loss improved from 1.03548 to 1.01560, saving model to Saved model files\\audio_classification2.hdf5\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.0718 - accuracy: 0.7141 - val_loss: 1.0156 - val_accuracy: 0.7476\n",
      "Epoch 59/375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91/110 [=======================>......] - ETA: 0s - loss: 1.0347 - accuracy: 0.7150\n",
      "Epoch 59: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0244 - accuracy: 0.7145 - val_loss: 1.0272 - val_accuracy: 0.7430\n",
      "Epoch 60/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0388 - accuracy: 0.7148\n",
      "Epoch 60: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0471 - accuracy: 0.7108 - val_loss: 1.0792 - val_accuracy: 0.7459\n",
      "Epoch 61/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.1205 - accuracy: 0.7159\n",
      "Epoch 61: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1160 - accuracy: 0.7142 - val_loss: 1.0658 - val_accuracy: 0.7579\n",
      "Epoch 62/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0453 - accuracy: 0.7027\n",
      "Epoch 62: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0435 - accuracy: 0.7029 - val_loss: 1.0498 - val_accuracy: 0.7333\n",
      "Epoch 63/375\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 1.0265 - accuracy: 0.7160\n",
      "Epoch 63: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0194 - accuracy: 0.7184 - val_loss: 1.0659 - val_accuracy: 0.7390\n",
      "Epoch 64/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0508 - accuracy: 0.7068\n",
      "Epoch 64: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0384 - accuracy: 0.7099 - val_loss: 1.0807 - val_accuracy: 0.7430\n",
      "Epoch 65/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.9760 - accuracy: 0.7166\n",
      "Epoch 65: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9820 - accuracy: 0.7168 - val_loss: 1.0658 - val_accuracy: 0.7596\n",
      "Epoch 66/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0242 - accuracy: 0.7165\n",
      "Epoch 66: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0420 - accuracy: 0.7175 - val_loss: 1.0558 - val_accuracy: 0.7544\n",
      "Epoch 67/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0518 - accuracy: 0.7156\n",
      "Epoch 67: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0449 - accuracy: 0.7168 - val_loss: 1.0695 - val_accuracy: 0.7550\n",
      "Epoch 68/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9874 - accuracy: 0.7179\n",
      "Epoch 68: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9843 - accuracy: 0.7188 - val_loss: 1.0664 - val_accuracy: 0.7499\n",
      "Epoch 69/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.7311\n",
      "Epoch 69: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0350 - accuracy: 0.7309 - val_loss: 1.0661 - val_accuracy: 0.7527\n",
      "Epoch 70/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0178 - accuracy: 0.7068\n",
      "Epoch 70: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0160 - accuracy: 0.7081 - val_loss: 1.0737 - val_accuracy: 0.7384\n",
      "Epoch 71/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0048 - accuracy: 0.7109\n",
      "Epoch 71: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0184 - accuracy: 0.7092 - val_loss: 1.0605 - val_accuracy: 0.7539\n",
      "Epoch 72/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9847 - accuracy: 0.7205\n",
      "Epoch 72: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9841 - accuracy: 0.7191 - val_loss: 1.0850 - val_accuracy: 0.7539\n",
      "Epoch 73/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0473 - accuracy: 0.7110\n",
      "Epoch 73: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0412 - accuracy: 0.7130 - val_loss: 1.0806 - val_accuracy: 0.7527\n",
      "Epoch 74/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0124 - accuracy: 0.7173\n",
      "Epoch 74: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9996 - accuracy: 0.7201 - val_loss: 1.0496 - val_accuracy: 0.7682\n",
      "Epoch 75/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.9853 - accuracy: 0.7194\n",
      "Epoch 75: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9768 - accuracy: 0.7218 - val_loss: 1.0597 - val_accuracy: 0.7430\n",
      "Epoch 76/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0078 - accuracy: 0.7156\n",
      "Epoch 76: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0004 - accuracy: 0.7161 - val_loss: 1.0875 - val_accuracy: 0.7516\n",
      "Epoch 77/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.9841 - accuracy: 0.7200\n",
      "Epoch 77: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9878 - accuracy: 0.7175 - val_loss: 1.1356 - val_accuracy: 0.7516\n",
      "Epoch 78/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0209 - accuracy: 0.7074\n",
      "Epoch 78: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0130 - accuracy: 0.7104 - val_loss: 1.1243 - val_accuracy: 0.7487\n",
      "Epoch 79/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.1093 - accuracy: 0.7103\n",
      "Epoch 79: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1127 - accuracy: 0.7104 - val_loss: 1.1128 - val_accuracy: 0.7504\n",
      "Epoch 80/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0977 - accuracy: 0.7101\n",
      "Epoch 80: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0957 - accuracy: 0.7097 - val_loss: 1.0746 - val_accuracy: 0.7504\n",
      "Epoch 81/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9718 - accuracy: 0.7122\n",
      "Epoch 81: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9766 - accuracy: 0.7128 - val_loss: 1.0860 - val_accuracy: 0.7396\n",
      "Epoch 82/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0207 - accuracy: 0.7142\n",
      "Epoch 82: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0245 - accuracy: 0.7135 - val_loss: 1.0632 - val_accuracy: 0.7510\n",
      "Epoch 83/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.9838 - accuracy: 0.7222\n",
      "Epoch 83: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0001 - accuracy: 0.7181 - val_loss: 1.1132 - val_accuracy: 0.7556\n",
      "Epoch 84/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0779 - accuracy: 0.7126\n",
      "Epoch 84: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0657 - accuracy: 0.7128 - val_loss: 1.0643 - val_accuracy: 0.7407\n",
      "Epoch 85/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.1193 - accuracy: 0.7034\n",
      "Epoch 85: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0981 - accuracy: 0.7062 - val_loss: 1.0632 - val_accuracy: 0.7361\n",
      "Epoch 86/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.7142\n",
      "Epoch 86: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0040 - accuracy: 0.7144 - val_loss: 1.1021 - val_accuracy: 0.7476\n",
      "Epoch 87/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0894 - accuracy: 0.7161\n",
      "Epoch 87: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0932 - accuracy: 0.7145 - val_loss: 1.0754 - val_accuracy: 0.7436\n",
      "Epoch 88/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0064 - accuracy: 0.7194\n",
      "Epoch 88: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0111 - accuracy: 0.7214 - val_loss: 1.0822 - val_accuracy: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9889 - accuracy: 0.7174\n",
      "Epoch 89: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9920 - accuracy: 0.7170 - val_loss: 1.0962 - val_accuracy: 0.7533\n",
      "Epoch 90/375\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 1.0568 - accuracy: 0.7191\n",
      "Epoch 90: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0541 - accuracy: 0.7207 - val_loss: 1.0793 - val_accuracy: 0.7607\n",
      "Epoch 91/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.1064 - accuracy: 0.7173\n",
      "Epoch 91: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1207 - accuracy: 0.7154 - val_loss: 1.0838 - val_accuracy: 0.7459\n",
      "Epoch 92/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0920 - accuracy: 0.7063\n",
      "Epoch 92: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0927 - accuracy: 0.7058 - val_loss: 1.0676 - val_accuracy: 0.7453\n",
      "Epoch 93/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0120 - accuracy: 0.7127\n",
      "Epoch 93: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0118 - accuracy: 0.7127 - val_loss: 1.0403 - val_accuracy: 0.7573\n",
      "Epoch 94/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0845 - accuracy: 0.7121\n",
      "Epoch 94: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0878 - accuracy: 0.7111 - val_loss: 1.0556 - val_accuracy: 0.7436\n",
      "Epoch 95/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.7084\n",
      "Epoch 95: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0372 - accuracy: 0.7087 - val_loss: 1.0868 - val_accuracy: 0.7378\n",
      "Epoch 96/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.7094\n",
      "Epoch 96: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0344 - accuracy: 0.7095 - val_loss: 1.0635 - val_accuracy: 0.7584\n",
      "Epoch 97/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0143 - accuracy: 0.7092\n",
      "Epoch 97: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0047 - accuracy: 0.7108 - val_loss: 1.1105 - val_accuracy: 0.7453\n",
      "Epoch 98/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9725 - accuracy: 0.7147\n",
      "Epoch 98: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9709 - accuracy: 0.7151 - val_loss: 1.0185 - val_accuracy: 0.7562\n",
      "Epoch 99/375\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 1.0477 - accuracy: 0.7111\n",
      "Epoch 99: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0472 - accuracy: 0.7132 - val_loss: 1.0700 - val_accuracy: 0.7481\n",
      "Epoch 100/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0067 - accuracy: 0.7172\n",
      "Epoch 100: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0187 - accuracy: 0.7188 - val_loss: 1.0733 - val_accuracy: 0.7527\n",
      "Epoch 101/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0209 - accuracy: 0.7098\n",
      "Epoch 101: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0307 - accuracy: 0.7094 - val_loss: 1.0618 - val_accuracy: 0.7378\n",
      "Epoch 102/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0816 - accuracy: 0.7127\n",
      "Epoch 102: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0777 - accuracy: 0.7117 - val_loss: 1.0359 - val_accuracy: 0.7521\n",
      "Epoch 103/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0750 - accuracy: 0.7160\n",
      "Epoch 103: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0630 - accuracy: 0.7152 - val_loss: 1.0682 - val_accuracy: 0.7556\n",
      "Epoch 104/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0703 - accuracy: 0.7138\n",
      "Epoch 104: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0704 - accuracy: 0.7144 - val_loss: 1.0584 - val_accuracy: 0.7539\n",
      "Epoch 105/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.7212\n",
      "Epoch 105: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0185 - accuracy: 0.7204 - val_loss: 1.0558 - val_accuracy: 0.7607\n",
      "Epoch 106/375\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 1.0738 - accuracy: 0.7191\n",
      "Epoch 106: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0545 - accuracy: 0.7203 - val_loss: 1.0728 - val_accuracy: 0.7584\n",
      "Epoch 107/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0838 - accuracy: 0.7134\n",
      "Epoch 107: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0766 - accuracy: 0.7142 - val_loss: 1.0740 - val_accuracy: 0.7407\n",
      "Epoch 108/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0308 - accuracy: 0.7125\n",
      "Epoch 108: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0826 - accuracy: 0.7101 - val_loss: 1.1018 - val_accuracy: 0.7407\n",
      "Epoch 109/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.9913 - accuracy: 0.7082\n",
      "Epoch 109: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9904 - accuracy: 0.7082 - val_loss: 1.1012 - val_accuracy: 0.7527\n",
      "Epoch 110/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.1430 - accuracy: 0.7096\n",
      "Epoch 110: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1478 - accuracy: 0.7099 - val_loss: 1.0933 - val_accuracy: 0.7533\n",
      "Epoch 111/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.9509 - accuracy: 0.7198\n",
      "Epoch 111: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9648 - accuracy: 0.7215 - val_loss: 1.0556 - val_accuracy: 0.7579\n",
      "Epoch 112/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.9609 - accuracy: 0.7178\n",
      "Epoch 112: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9705 - accuracy: 0.7174 - val_loss: 1.0844 - val_accuracy: 0.7544\n",
      "Epoch 113/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.7148\n",
      "Epoch 113: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0373 - accuracy: 0.7145 - val_loss: 1.0770 - val_accuracy: 0.7556\n",
      "Epoch 114/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0235 - accuracy: 0.7162\n",
      "Epoch 114: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0182 - accuracy: 0.7164 - val_loss: 1.0926 - val_accuracy: 0.7476\n",
      "Epoch 115/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0347 - accuracy: 0.7100\n",
      "Epoch 115: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0461 - accuracy: 0.7105 - val_loss: 1.1118 - val_accuracy: 0.7315\n",
      "Epoch 116/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0176 - accuracy: 0.7096\n",
      "Epoch 116: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0172 - accuracy: 0.7094 - val_loss: 1.0511 - val_accuracy: 0.7447\n",
      "Epoch 117/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.1531 - accuracy: 0.7135\n",
      "Epoch 117: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.1496 - accuracy: 0.7144 - val_loss: 1.1207 - val_accuracy: 0.7527\n",
      "Epoch 118/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0006 - accuracy: 0.7109\n",
      "Epoch 118: val_loss did not improve from 1.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0155 - accuracy: 0.7098 - val_loss: 1.1109 - val_accuracy: 0.7573\n",
      "Epoch 119/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0771 - accuracy: 0.7095\n",
      "Epoch 119: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0927 - accuracy: 0.7084 - val_loss: 1.0429 - val_accuracy: 0.7567\n",
      "Epoch 120/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9787 - accuracy: 0.7183\n",
      "Epoch 120: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9790 - accuracy: 0.7183 - val_loss: 1.1272 - val_accuracy: 0.7476\n",
      "Epoch 121/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0544 - accuracy: 0.7177\n",
      "Epoch 121: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0492 - accuracy: 0.7173 - val_loss: 1.1044 - val_accuracy: 0.7521\n",
      "Epoch 122/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0318 - accuracy: 0.7123\n",
      "Epoch 122: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0271 - accuracy: 0.7102 - val_loss: 1.1191 - val_accuracy: 0.7453\n",
      "Epoch 123/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.9912 - accuracy: 0.7233\n",
      "Epoch 123: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9912 - accuracy: 0.7233 - val_loss: 1.1297 - val_accuracy: 0.7407\n",
      "Epoch 124/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.7129\n",
      "Epoch 124: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0134 - accuracy: 0.7142 - val_loss: 1.1115 - val_accuracy: 0.7418\n",
      "Epoch 125/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0566 - accuracy: 0.7234\n",
      "Epoch 125: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0567 - accuracy: 0.7227 - val_loss: 1.1700 - val_accuracy: 0.7476\n",
      "Epoch 126/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0436 - accuracy: 0.7113\n",
      "Epoch 126: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0464 - accuracy: 0.7092 - val_loss: 1.1796 - val_accuracy: 0.7413\n",
      "Epoch 127/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0159 - accuracy: 0.7136\n",
      "Epoch 127: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0366 - accuracy: 0.7114 - val_loss: 1.1726 - val_accuracy: 0.7516\n",
      "Epoch 128/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9969 - accuracy: 0.7176\n",
      "Epoch 128: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9917 - accuracy: 0.7193 - val_loss: 1.1280 - val_accuracy: 0.7464\n",
      "Epoch 129/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0141 - accuracy: 0.7210\n",
      "Epoch 129: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0201 - accuracy: 0.7201 - val_loss: 1.1518 - val_accuracy: 0.7499\n",
      "Epoch 130/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9550 - accuracy: 0.7136\n",
      "Epoch 130: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9512 - accuracy: 0.7135 - val_loss: 1.0997 - val_accuracy: 0.7584\n",
      "Epoch 131/375\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 1.0331 - accuracy: 0.7115\n",
      "Epoch 131: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0235 - accuracy: 0.7117 - val_loss: 1.1749 - val_accuracy: 0.7493\n",
      "Epoch 132/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0789 - accuracy: 0.7227\n",
      "Epoch 132: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0646 - accuracy: 0.7215 - val_loss: 1.1201 - val_accuracy: 0.7378\n",
      "Epoch 133/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0306 - accuracy: 0.7177\n",
      "Epoch 133: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0272 - accuracy: 0.7175 - val_loss: 1.0772 - val_accuracy: 0.7642\n",
      "Epoch 134/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0350 - accuracy: 0.7200\n",
      "Epoch 134: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0269 - accuracy: 0.7188 - val_loss: 1.1005 - val_accuracy: 0.7527\n",
      "Epoch 135/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0716 - accuracy: 0.7127\n",
      "Epoch 135: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0614 - accuracy: 0.7131 - val_loss: 1.1222 - val_accuracy: 0.7499\n",
      "Epoch 136/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0347 - accuracy: 0.7117\n",
      "Epoch 136: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0332 - accuracy: 0.7145 - val_loss: 1.0995 - val_accuracy: 0.7567\n",
      "Epoch 137/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0310 - accuracy: 0.7201\n",
      "Epoch 137: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0273 - accuracy: 0.7185 - val_loss: 1.0951 - val_accuracy: 0.7470\n",
      "Epoch 138/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0442 - accuracy: 0.7099\n",
      "Epoch 138: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0434 - accuracy: 0.7098 - val_loss: 1.0808 - val_accuracy: 0.7418\n",
      "Epoch 139/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0847 - accuracy: 0.7246\n",
      "Epoch 139: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0919 - accuracy: 0.7246 - val_loss: 1.1503 - val_accuracy: 0.7378\n",
      "Epoch 140/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0050 - accuracy: 0.7135\n",
      "Epoch 140: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0018 - accuracy: 0.7145 - val_loss: 1.1343 - val_accuracy: 0.7533\n",
      "Epoch 141/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9634 - accuracy: 0.7255\n",
      "Epoch 141: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9769 - accuracy: 0.7264 - val_loss: 1.1117 - val_accuracy: 0.7573\n",
      "Epoch 142/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0262 - accuracy: 0.7109\n",
      "Epoch 142: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0207 - accuracy: 0.7110 - val_loss: 1.0675 - val_accuracy: 0.7464\n",
      "Epoch 143/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0218 - accuracy: 0.7144\n",
      "Epoch 143: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0341 - accuracy: 0.7177 - val_loss: 1.1106 - val_accuracy: 0.7413\n",
      "Epoch 144/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0545 - accuracy: 0.7157\n",
      "Epoch 144: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0470 - accuracy: 0.7167 - val_loss: 1.1356 - val_accuracy: 0.7413\n",
      "Epoch 145/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0279 - accuracy: 0.7150\n",
      "Epoch 145: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0292 - accuracy: 0.7150 - val_loss: 1.1072 - val_accuracy: 0.7453\n",
      "Epoch 146/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9822 - accuracy: 0.7151\n",
      "Epoch 146: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9809 - accuracy: 0.7158 - val_loss: 1.0983 - val_accuracy: 0.7562\n",
      "Epoch 147/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9838 - accuracy: 0.7149\n",
      "Epoch 147: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9830 - accuracy: 0.7151 - val_loss: 1.1339 - val_accuracy: 0.7544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0429 - accuracy: 0.7174\n",
      "Epoch 148: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0353 - accuracy: 0.7164 - val_loss: 1.0781 - val_accuracy: 0.7401\n",
      "Epoch 149/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0365 - accuracy: 0.7152\n",
      "Epoch 149: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.0456 - accuracy: 0.7114 - val_loss: 1.0672 - val_accuracy: 0.7533\n",
      "Epoch 150/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.7154\n",
      "Epoch 150: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0248 - accuracy: 0.7160 - val_loss: 1.1584 - val_accuracy: 0.7447\n",
      "Epoch 151/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0013 - accuracy: 0.7101\n",
      "Epoch 151: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0158 - accuracy: 0.7091 - val_loss: 1.1019 - val_accuracy: 0.7533\n",
      "Epoch 152/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0585 - accuracy: 0.7130\n",
      "Epoch 152: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0579 - accuracy: 0.7132 - val_loss: 1.0767 - val_accuracy: 0.7562\n",
      "Epoch 153/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0131 - accuracy: 0.7135\n",
      "Epoch 153: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9980 - accuracy: 0.7171 - val_loss: 1.0938 - val_accuracy: 0.7619\n",
      "Epoch 154/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0088 - accuracy: 0.7167\n",
      "Epoch 154: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 1.0085 - accuracy: 0.7151 - val_loss: 1.0934 - val_accuracy: 0.7390\n",
      "Epoch 155/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9938 - accuracy: 0.7131\n",
      "Epoch 155: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9937 - accuracy: 0.7127 - val_loss: 1.1008 - val_accuracy: 0.7504\n",
      "Epoch 156/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0329 - accuracy: 0.7113\n",
      "Epoch 156: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0377 - accuracy: 0.7097 - val_loss: 1.0380 - val_accuracy: 0.7424\n",
      "Epoch 157/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0542 - accuracy: 0.7124\n",
      "Epoch 157: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0556 - accuracy: 0.7122 - val_loss: 1.0447 - val_accuracy: 0.7481\n",
      "Epoch 158/375\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 1.1072 - accuracy: 0.7122\n",
      "Epoch 158: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.1168 - accuracy: 0.7157 - val_loss: 1.0483 - val_accuracy: 0.7550\n",
      "Epoch 159/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0061 - accuracy: 0.7243\n",
      "Epoch 159: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0033 - accuracy: 0.7246 - val_loss: 1.0767 - val_accuracy: 0.7533\n",
      "Epoch 160/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0360 - accuracy: 0.7239\n",
      "Epoch 160: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0429 - accuracy: 0.7251 - val_loss: 1.0911 - val_accuracy: 0.7453\n",
      "Epoch 161/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0330 - accuracy: 0.7155\n",
      "Epoch 161: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0335 - accuracy: 0.7155 - val_loss: 1.0266 - val_accuracy: 0.7619\n",
      "Epoch 162/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0158 - accuracy: 0.7183\n",
      "Epoch 162: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0269 - accuracy: 0.7155 - val_loss: 1.0684 - val_accuracy: 0.7476\n",
      "Epoch 163/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.9605 - accuracy: 0.7251\n",
      "Epoch 163: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 0.9702 - accuracy: 0.7244 - val_loss: 1.0442 - val_accuracy: 0.7596\n",
      "Epoch 164/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0038 - accuracy: 0.7204\n",
      "Epoch 164: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0026 - accuracy: 0.7198 - val_loss: 1.0646 - val_accuracy: 0.7470\n",
      "Epoch 165/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9787 - accuracy: 0.7206\n",
      "Epoch 165: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9786 - accuracy: 0.7204 - val_loss: 1.0878 - val_accuracy: 0.7476\n",
      "Epoch 166/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0345 - accuracy: 0.7225\n",
      "Epoch 166: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0279 - accuracy: 0.7236 - val_loss: 1.0764 - val_accuracy: 0.7590\n",
      "Epoch 167/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9800 - accuracy: 0.7218\n",
      "Epoch 167: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9797 - accuracy: 0.7217 - val_loss: 1.0912 - val_accuracy: 0.7499\n",
      "Epoch 168/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9729 - accuracy: 0.7192\n",
      "Epoch 168: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9782 - accuracy: 0.7191 - val_loss: 1.0896 - val_accuracy: 0.7521\n",
      "Epoch 169/375\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 1.1376 - accuracy: 0.7168\n",
      "Epoch 169: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1051 - accuracy: 0.7191 - val_loss: 1.0956 - val_accuracy: 0.7539\n",
      "Epoch 170/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.9409 - accuracy: 0.7241\n",
      "Epoch 170: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9399 - accuracy: 0.7234 - val_loss: 1.1335 - val_accuracy: 0.7487\n",
      "Epoch 171/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0966 - accuracy: 0.7130\n",
      "Epoch 171: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0883 - accuracy: 0.7138 - val_loss: 1.1162 - val_accuracy: 0.7567\n",
      "Epoch 172/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.9990 - accuracy: 0.7251\n",
      "Epoch 172: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0007 - accuracy: 0.7250 - val_loss: 1.1540 - val_accuracy: 0.7590\n",
      "Epoch 173/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0006 - accuracy: 0.7116\n",
      "Epoch 173: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0010 - accuracy: 0.7118 - val_loss: 1.1048 - val_accuracy: 0.7418\n",
      "Epoch 174/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.9967 - accuracy: 0.7144\n",
      "Epoch 174: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9872 - accuracy: 0.7164 - val_loss: 1.0994 - val_accuracy: 0.7533\n",
      "Epoch 175/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9823 - accuracy: 0.7232\n",
      "Epoch 175: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9830 - accuracy: 0.7237 - val_loss: 1.1489 - val_accuracy: 0.7556\n",
      "Epoch 176/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0356 - accuracy: 0.7194\n",
      "Epoch 176: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0445 - accuracy: 0.7205 - val_loss: 1.0664 - val_accuracy: 0.7624\n",
      "Epoch 177/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0371 - accuracy: 0.7089\n",
      "Epoch 177: val_loss did not improve from 1.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0307 - accuracy: 0.7107 - val_loss: 1.0920 - val_accuracy: 0.7521\n",
      "Epoch 178/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0155 - accuracy: 0.7186\n",
      "Epoch 178: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0162 - accuracy: 0.7184 - val_loss: 1.1206 - val_accuracy: 0.7539\n",
      "Epoch 179/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.9762 - accuracy: 0.7234\n",
      "Epoch 179: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9794 - accuracy: 0.7233 - val_loss: 1.1145 - val_accuracy: 0.7396\n",
      "Epoch 180/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0340 - accuracy: 0.7180\n",
      "Epoch 180: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0240 - accuracy: 0.7181 - val_loss: 1.1103 - val_accuracy: 0.7544\n",
      "Epoch 181/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0934 - accuracy: 0.7049\n",
      "Epoch 181: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.1030 - accuracy: 0.7045 - val_loss: 1.1402 - val_accuracy: 0.7418\n",
      "Epoch 182/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0248 - accuracy: 0.7183\n",
      "Epoch 182: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0294 - accuracy: 0.7171 - val_loss: 1.1151 - val_accuracy: 0.7499\n",
      "Epoch 183/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9987 - accuracy: 0.7182\n",
      "Epoch 183: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9979 - accuracy: 0.7177 - val_loss: 1.1374 - val_accuracy: 0.7338\n",
      "Epoch 184/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0437 - accuracy: 0.7198\n",
      "Epoch 184: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 1.0508 - accuracy: 0.7205 - val_loss: 1.1292 - val_accuracy: 0.7453\n",
      "Epoch 185/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9841 - accuracy: 0.7183\n",
      "Epoch 185: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9818 - accuracy: 0.7180 - val_loss: 1.1062 - val_accuracy: 0.7659\n",
      "Epoch 186/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0016 - accuracy: 0.7182\n",
      "Epoch 186: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9944 - accuracy: 0.7180 - val_loss: 1.1574 - val_accuracy: 0.7596\n",
      "Epoch 187/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.9899 - accuracy: 0.7205\n",
      "Epoch 187: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9953 - accuracy: 0.7200 - val_loss: 1.1471 - val_accuracy: 0.7510\n",
      "Epoch 188/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0417 - accuracy: 0.7194\n",
      "Epoch 188: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0278 - accuracy: 0.7187 - val_loss: 1.0985 - val_accuracy: 0.7602\n",
      "Epoch 189/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0071 - accuracy: 0.7054\n",
      "Epoch 189: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0020 - accuracy: 0.7069 - val_loss: 1.1643 - val_accuracy: 0.7459\n",
      "Epoch 190/375\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 1.0677 - accuracy: 0.7144\n",
      "Epoch 190: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0559 - accuracy: 0.7158 - val_loss: 1.0734 - val_accuracy: 0.7453\n",
      "Epoch 191/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0860 - accuracy: 0.7132\n",
      "Epoch 191: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0860 - accuracy: 0.7132 - val_loss: 1.1467 - val_accuracy: 0.7373\n",
      "Epoch 192/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0457 - accuracy: 0.7199\n",
      "Epoch 192: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 1.0412 - accuracy: 0.7190 - val_loss: 1.1029 - val_accuracy: 0.7464\n",
      "Epoch 193/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0413 - accuracy: 0.7178\n",
      "Epoch 193: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0299 - accuracy: 0.7178 - val_loss: 1.1081 - val_accuracy: 0.7493\n",
      "Epoch 194/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9595 - accuracy: 0.7210\n",
      "Epoch 194: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9710 - accuracy: 0.7195 - val_loss: 1.1445 - val_accuracy: 0.7487\n",
      "Epoch 195/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0342 - accuracy: 0.7097\n",
      "Epoch 195: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0281 - accuracy: 0.7117 - val_loss: 1.1702 - val_accuracy: 0.7516\n",
      "Epoch 196/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0570 - accuracy: 0.7167\n",
      "Epoch 196: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0570 - accuracy: 0.7167 - val_loss: 1.1129 - val_accuracy: 0.7579\n",
      "Epoch 197/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0143 - accuracy: 0.7116\n",
      "Epoch 197: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0324 - accuracy: 0.7112 - val_loss: 1.1671 - val_accuracy: 0.7436\n",
      "Epoch 198/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0607 - accuracy: 0.7196\n",
      "Epoch 198: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0590 - accuracy: 0.7200 - val_loss: 1.1689 - val_accuracy: 0.7413\n",
      "Epoch 199/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0237 - accuracy: 0.7243\n",
      "Epoch 199: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0202 - accuracy: 0.7224 - val_loss: 1.1666 - val_accuracy: 0.7447\n",
      "Epoch 200/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0319 - accuracy: 0.7150\n",
      "Epoch 200: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0335 - accuracy: 0.7141 - val_loss: 1.1325 - val_accuracy: 0.7373\n",
      "Epoch 201/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0287 - accuracy: 0.7170\n",
      "Epoch 201: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0268 - accuracy: 0.7173 - val_loss: 1.1609 - val_accuracy: 0.7470\n",
      "Epoch 202/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0291 - accuracy: 0.7137\n",
      "Epoch 202: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0312 - accuracy: 0.7151 - val_loss: 1.0787 - val_accuracy: 0.7590\n",
      "Epoch 203/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.9722 - accuracy: 0.7198\n",
      "Epoch 203: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.9749 - accuracy: 0.7200 - val_loss: 1.1069 - val_accuracy: 0.7487\n",
      "Epoch 204/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0159 - accuracy: 0.7175\n",
      "Epoch 204: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0275 - accuracy: 0.7198 - val_loss: 1.1176 - val_accuracy: 0.7550\n",
      "Epoch 205/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0663 - accuracy: 0.7198\n",
      "Epoch 205: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0660 - accuracy: 0.7195 - val_loss: 1.1351 - val_accuracy: 0.7476\n",
      "Epoch 206/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0064 - accuracy: 0.7140\n",
      "Epoch 206: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.9971 - accuracy: 0.7150 - val_loss: 1.0954 - val_accuracy: 0.7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.1093 - accuracy: 0.7165\n",
      "Epoch 207: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1079 - accuracy: 0.7168 - val_loss: 1.1283 - val_accuracy: 0.7487\n",
      "Epoch 208/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0028 - accuracy: 0.7198\n",
      "Epoch 208: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0036 - accuracy: 0.7191 - val_loss: 1.0950 - val_accuracy: 0.7573\n",
      "Epoch 209/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0080 - accuracy: 0.7158\n",
      "Epoch 209: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0080 - accuracy: 0.7158 - val_loss: 1.1414 - val_accuracy: 0.7481\n",
      "Epoch 210/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 0.9895 - accuracy: 0.7176\n",
      "Epoch 210: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9929 - accuracy: 0.7147 - val_loss: 1.1668 - val_accuracy: 0.7350\n",
      "Epoch 211/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0864 - accuracy: 0.7114\n",
      "Epoch 211: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0864 - accuracy: 0.7114 - val_loss: 1.1410 - val_accuracy: 0.7401\n",
      "Epoch 212/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0567 - accuracy: 0.7095\n",
      "Epoch 212: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0556 - accuracy: 0.7099 - val_loss: 1.1015 - val_accuracy: 0.7602\n",
      "Epoch 213/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.7113\n",
      "Epoch 213: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0232 - accuracy: 0.7112 - val_loss: 1.1418 - val_accuracy: 0.7487\n",
      "Epoch 214/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0720 - accuracy: 0.7205\n",
      "Epoch 214: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0706 - accuracy: 0.7231 - val_loss: 1.1145 - val_accuracy: 0.7573\n",
      "Epoch 215/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0190 - accuracy: 0.7184\n",
      "Epoch 215: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0198 - accuracy: 0.7174 - val_loss: 1.1083 - val_accuracy: 0.7550\n",
      "Epoch 216/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0495 - accuracy: 0.7179\n",
      "Epoch 216: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0494 - accuracy: 0.7180 - val_loss: 1.1450 - val_accuracy: 0.7521\n",
      "Epoch 217/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0397 - accuracy: 0.7128\n",
      "Epoch 217: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0392 - accuracy: 0.7148 - val_loss: 1.0946 - val_accuracy: 0.7367\n",
      "Epoch 218/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9871 - accuracy: 0.7186\n",
      "Epoch 218: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9871 - accuracy: 0.7205 - val_loss: 1.0748 - val_accuracy: 0.7499\n",
      "Epoch 219/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0206 - accuracy: 0.7195\n",
      "Epoch 219: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0246 - accuracy: 0.7173 - val_loss: 1.0612 - val_accuracy: 0.7493\n",
      "Epoch 220/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0320 - accuracy: 0.7180\n",
      "Epoch 220: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0318 - accuracy: 0.7170 - val_loss: 1.0959 - val_accuracy: 0.7401\n",
      "Epoch 221/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0218 - accuracy: 0.7136\n",
      "Epoch 221: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0340 - accuracy: 0.7124 - val_loss: 1.0579 - val_accuracy: 0.7430\n",
      "Epoch 222/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.7173\n",
      "Epoch 222: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0390 - accuracy: 0.7177 - val_loss: 1.0881 - val_accuracy: 0.7481\n",
      "Epoch 223/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0574 - accuracy: 0.7067\n",
      "Epoch 223: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0498 - accuracy: 0.7074 - val_loss: 1.0626 - val_accuracy: 0.7579\n",
      "Epoch 224/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0135 - accuracy: 0.7164\n",
      "Epoch 224: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0151 - accuracy: 0.7168 - val_loss: 1.0814 - val_accuracy: 0.7447\n",
      "Epoch 225/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0021 - accuracy: 0.7172\n",
      "Epoch 225: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0068 - accuracy: 0.7135 - val_loss: 1.1011 - val_accuracy: 0.7567\n",
      "Epoch 226/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0771 - accuracy: 0.7106\n",
      "Epoch 226: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0811 - accuracy: 0.7112 - val_loss: 1.0375 - val_accuracy: 0.7544\n",
      "Epoch 227/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0458 - accuracy: 0.7155\n",
      "Epoch 227: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0478 - accuracy: 0.7154 - val_loss: 1.0536 - val_accuracy: 0.7390\n",
      "Epoch 228/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0160 - accuracy: 0.7135\n",
      "Epoch 228: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0206 - accuracy: 0.7118 - val_loss: 1.0671 - val_accuracy: 0.7487\n",
      "Epoch 229/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9654 - accuracy: 0.7182\n",
      "Epoch 229: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9639 - accuracy: 0.7178 - val_loss: 1.0977 - val_accuracy: 0.7550\n",
      "Epoch 230/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0282 - accuracy: 0.7123\n",
      "Epoch 230: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0340 - accuracy: 0.7135 - val_loss: 1.0638 - val_accuracy: 0.7510\n",
      "Epoch 231/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0385 - accuracy: 0.7170\n",
      "Epoch 231: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0424 - accuracy: 0.7161 - val_loss: 1.1146 - val_accuracy: 0.7487\n",
      "Epoch 232/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0337 - accuracy: 0.7078\n",
      "Epoch 232: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0317 - accuracy: 0.7092 - val_loss: 1.0682 - val_accuracy: 0.7527\n",
      "Epoch 233/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9694 - accuracy: 0.7179\n",
      "Epoch 233: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9906 - accuracy: 0.7158 - val_loss: 1.0457 - val_accuracy: 0.7493\n",
      "Epoch 234/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0304 - accuracy: 0.7111\n",
      "Epoch 234: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0242 - accuracy: 0.7102 - val_loss: 1.0897 - val_accuracy: 0.7550\n",
      "Epoch 235/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9894 - accuracy: 0.7264\n",
      "Epoch 235: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9870 - accuracy: 0.7263 - val_loss: 1.1082 - val_accuracy: 0.7464\n",
      "Epoch 236/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0550 - accuracy: 0.7177\n",
      "Epoch 236: val_loss did not improve from 1.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0643 - accuracy: 0.7185 - val_loss: 1.0658 - val_accuracy: 0.7499\n",
      "Epoch 237/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0464 - accuracy: 0.7158\n",
      "Epoch 237: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0575 - accuracy: 0.7140 - val_loss: 1.0814 - val_accuracy: 0.7539\n",
      "Epoch 238/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0475 - accuracy: 0.7244\n",
      "Epoch 238: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0510 - accuracy: 0.7221 - val_loss: 1.0378 - val_accuracy: 0.7665\n",
      "Epoch 239/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.7125\n",
      "Epoch 239: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0177 - accuracy: 0.7128 - val_loss: 1.0733 - val_accuracy: 0.7447\n",
      "Epoch 240/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0318 - accuracy: 0.7172\n",
      "Epoch 240: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0335 - accuracy: 0.7184 - val_loss: 1.1004 - val_accuracy: 0.7584\n",
      "Epoch 241/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0155 - accuracy: 0.7157\n",
      "Epoch 241: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0122 - accuracy: 0.7161 - val_loss: 1.0566 - val_accuracy: 0.7539\n",
      "Epoch 242/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0196 - accuracy: 0.7173\n",
      "Epoch 242: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0181 - accuracy: 0.7170 - val_loss: 1.0656 - val_accuracy: 0.7521\n",
      "Epoch 243/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0668 - accuracy: 0.7155\n",
      "Epoch 243: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0596 - accuracy: 0.7160 - val_loss: 1.0969 - val_accuracy: 0.7544\n",
      "Epoch 244/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.1108 - accuracy: 0.7166\n",
      "Epoch 244: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.1010 - accuracy: 0.7194 - val_loss: 1.1034 - val_accuracy: 0.7562\n",
      "Epoch 245/375\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0717 - accuracy: 0.7254\n",
      "Epoch 245: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0541 - accuracy: 0.7233 - val_loss: 1.0805 - val_accuracy: 0.7516\n",
      "Epoch 246/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.2099 - accuracy: 0.7076\n",
      "Epoch 246: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.2079 - accuracy: 0.7082 - val_loss: 1.0665 - val_accuracy: 0.7493\n",
      "Epoch 247/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9838 - accuracy: 0.7163\n",
      "Epoch 247: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9840 - accuracy: 0.7161 - val_loss: 1.1037 - val_accuracy: 0.7407\n",
      "Epoch 248/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0905 - accuracy: 0.7113\n",
      "Epoch 248: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0752 - accuracy: 0.7092 - val_loss: 1.0724 - val_accuracy: 0.7504\n",
      "Epoch 249/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0685 - accuracy: 0.7188\n",
      "Epoch 249: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0668 - accuracy: 0.7177 - val_loss: 1.1069 - val_accuracy: 0.7459\n",
      "Epoch 250/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0312 - accuracy: 0.7165\n",
      "Epoch 250: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0400 - accuracy: 0.7155 - val_loss: 1.1247 - val_accuracy: 0.7407\n",
      "Epoch 251/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0523 - accuracy: 0.7087\n",
      "Epoch 251: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0378 - accuracy: 0.7118 - val_loss: 1.0985 - val_accuracy: 0.7527\n",
      "Epoch 252/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0367 - accuracy: 0.7192\n",
      "Epoch 252: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0216 - accuracy: 0.7205 - val_loss: 1.0622 - val_accuracy: 0.7499\n",
      "Epoch 253/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0524 - accuracy: 0.7062\n",
      "Epoch 253: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0352 - accuracy: 0.7072 - val_loss: 1.0832 - val_accuracy: 0.7361\n",
      "Epoch 254/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0297 - accuracy: 0.7119\n",
      "Epoch 254: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0294 - accuracy: 0.7125 - val_loss: 1.0907 - val_accuracy: 0.7459\n",
      "Epoch 255/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0381 - accuracy: 0.7221\n",
      "Epoch 255: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0490 - accuracy: 0.7215 - val_loss: 1.0703 - val_accuracy: 0.7401\n",
      "Epoch 256/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0404 - accuracy: 0.7216\n",
      "Epoch 256: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0363 - accuracy: 0.7211 - val_loss: 1.0775 - val_accuracy: 0.7504\n",
      "Epoch 257/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.7136\n",
      "Epoch 257: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0158 - accuracy: 0.7137 - val_loss: 1.0719 - val_accuracy: 0.7487\n",
      "Epoch 258/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0006 - accuracy: 0.7112\n",
      "Epoch 258: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9997 - accuracy: 0.7105 - val_loss: 1.0992 - val_accuracy: 0.7499\n",
      "Epoch 259/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0292 - accuracy: 0.7118\n",
      "Epoch 259: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0292 - accuracy: 0.7118 - val_loss: 1.1211 - val_accuracy: 0.7584\n",
      "Epoch 260/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0325 - accuracy: 0.7125\n",
      "Epoch 260: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0328 - accuracy: 0.7122 - val_loss: 1.0466 - val_accuracy: 0.7602\n",
      "Epoch 261/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0642 - accuracy: 0.7153\n",
      "Epoch 261: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0591 - accuracy: 0.7135 - val_loss: 1.0668 - val_accuracy: 0.7407\n",
      "Epoch 262/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0636 - accuracy: 0.7147\n",
      "Epoch 262: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0617 - accuracy: 0.7134 - val_loss: 1.0828 - val_accuracy: 0.7567\n",
      "Epoch 263/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.1241 - accuracy: 0.7170\n",
      "Epoch 263: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.1182 - accuracy: 0.7183 - val_loss: 1.0914 - val_accuracy: 0.7390\n",
      "Epoch 264/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0046 - accuracy: 0.7148\n",
      "Epoch 264: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0049 - accuracy: 0.7147 - val_loss: 1.1401 - val_accuracy: 0.7481\n",
      "Epoch 265/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9982 - accuracy: 0.7144\n",
      "Epoch 265: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0067 - accuracy: 0.7135 - val_loss: 1.0959 - val_accuracy: 0.7476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0625 - accuracy: 0.7181\n",
      "Epoch 266: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0625 - accuracy: 0.7181 - val_loss: 1.0539 - val_accuracy: 0.7521\n",
      "Epoch 267/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0238 - accuracy: 0.7114\n",
      "Epoch 267: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0362 - accuracy: 0.7095 - val_loss: 1.1214 - val_accuracy: 0.7413\n",
      "Epoch 268/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.9990 - accuracy: 0.7186\n",
      "Epoch 268: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0035 - accuracy: 0.7175 - val_loss: 1.1473 - val_accuracy: 0.7293\n",
      "Epoch 269/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0919 - accuracy: 0.7027\n",
      "Epoch 269: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.1007 - accuracy: 0.7042 - val_loss: 1.1427 - val_accuracy: 0.7436\n",
      "Epoch 270/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.7124\n",
      "Epoch 270: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0501 - accuracy: 0.7127 - val_loss: 1.1231 - val_accuracy: 0.7418\n",
      "Epoch 271/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0174 - accuracy: 0.7184\n",
      "Epoch 271: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0161 - accuracy: 0.7181 - val_loss: 1.1094 - val_accuracy: 0.7355\n",
      "Epoch 272/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0413 - accuracy: 0.7052\n",
      "Epoch 272: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0381 - accuracy: 0.7067 - val_loss: 1.1660 - val_accuracy: 0.7413\n",
      "Epoch 273/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0034 - accuracy: 0.7161\n",
      "Epoch 273: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0120 - accuracy: 0.7170 - val_loss: 1.2043 - val_accuracy: 0.7327\n",
      "Epoch 274/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.7133\n",
      "Epoch 274: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9981 - accuracy: 0.7138 - val_loss: 1.1689 - val_accuracy: 0.7441\n",
      "Epoch 275/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.9663 - accuracy: 0.7231\n",
      "Epoch 275: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9659 - accuracy: 0.7230 - val_loss: 1.1451 - val_accuracy: 0.7504\n",
      "Epoch 276/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.7242\n",
      "Epoch 276: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0475 - accuracy: 0.7220 - val_loss: 1.1401 - val_accuracy: 0.7476\n",
      "Epoch 277/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0131 - accuracy: 0.7164\n",
      "Epoch 277: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0137 - accuracy: 0.7167 - val_loss: 1.0914 - val_accuracy: 0.7510\n",
      "Epoch 278/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.9823 - accuracy: 0.7214\n",
      "Epoch 278: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9850 - accuracy: 0.7194 - val_loss: 1.0845 - val_accuracy: 0.7487\n",
      "Epoch 279/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.9757 - accuracy: 0.7132\n",
      "Epoch 279: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9769 - accuracy: 0.7125 - val_loss: 1.1074 - val_accuracy: 0.7436\n",
      "Epoch 280/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0091 - accuracy: 0.7153\n",
      "Epoch 280: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0098 - accuracy: 0.7155 - val_loss: 1.0965 - val_accuracy: 0.7453\n",
      "Epoch 281/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 0.9801 - accuracy: 0.7239\n",
      "Epoch 281: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9886 - accuracy: 0.7214 - val_loss: 1.1256 - val_accuracy: 0.7430\n",
      "Epoch 282/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0625 - accuracy: 0.7128\n",
      "Epoch 282: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0562 - accuracy: 0.7134 - val_loss: 1.1493 - val_accuracy: 0.7464\n",
      "Epoch 283/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0289 - accuracy: 0.7207\n",
      "Epoch 283: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0289 - accuracy: 0.7207 - val_loss: 1.1392 - val_accuracy: 0.7401\n",
      "Epoch 284/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0523 - accuracy: 0.7162\n",
      "Epoch 284: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0537 - accuracy: 0.7170 - val_loss: 1.1328 - val_accuracy: 0.7424\n",
      "Epoch 285/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0195 - accuracy: 0.7155\n",
      "Epoch 285: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0159 - accuracy: 0.7174 - val_loss: 1.1353 - val_accuracy: 0.7584\n",
      "Epoch 286/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0297 - accuracy: 0.7184\n",
      "Epoch 286: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0515 - accuracy: 0.7190 - val_loss: 1.1008 - val_accuracy: 0.7470\n",
      "Epoch 287/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.7115\n",
      "Epoch 287: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0350 - accuracy: 0.7112 - val_loss: 1.1080 - val_accuracy: 0.7441\n",
      "Epoch 288/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0117 - accuracy: 0.7206\n",
      "Epoch 288: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0275 - accuracy: 0.7174 - val_loss: 1.1480 - val_accuracy: 0.7321\n",
      "Epoch 289/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0801 - accuracy: 0.7127\n",
      "Epoch 289: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0798 - accuracy: 0.7120 - val_loss: 1.1583 - val_accuracy: 0.7441\n",
      "Epoch 290/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.9802 - accuracy: 0.7250\n",
      "Epoch 290: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9889 - accuracy: 0.7228 - val_loss: 1.1829 - val_accuracy: 0.7441\n",
      "Epoch 291/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0924 - accuracy: 0.7170\n",
      "Epoch 291: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0909 - accuracy: 0.7162 - val_loss: 1.1756 - val_accuracy: 0.7355\n",
      "Epoch 292/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0605 - accuracy: 0.7105\n",
      "Epoch 292: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0684 - accuracy: 0.7092 - val_loss: 1.1106 - val_accuracy: 0.7355\n",
      "Epoch 293/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.1027 - accuracy: 0.7115\n",
      "Epoch 293: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0951 - accuracy: 0.7117 - val_loss: 1.0663 - val_accuracy: 0.7584\n",
      "Epoch 294/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0328 - accuracy: 0.7153\n",
      "Epoch 294: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0227 - accuracy: 0.7174 - val_loss: 1.0876 - val_accuracy: 0.7607\n",
      "Epoch 295/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.7163\n",
      "Epoch 295: val_loss did not improve from 1.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0208 - accuracy: 0.7167 - val_loss: 1.1207 - val_accuracy: 0.7487\n",
      "Epoch 296/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0455 - accuracy: 0.7119\n",
      "Epoch 296: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0362 - accuracy: 0.7140 - val_loss: 1.1152 - val_accuracy: 0.7430\n",
      "Epoch 297/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.1066 - accuracy: 0.7002\n",
      "Epoch 297: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0989 - accuracy: 0.7005 - val_loss: 1.0443 - val_accuracy: 0.7373\n",
      "Epoch 298/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0107 - accuracy: 0.7171\n",
      "Epoch 298: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0268 - accuracy: 0.7167 - val_loss: 1.1103 - val_accuracy: 0.7287\n",
      "Epoch 299/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0890 - accuracy: 0.7124\n",
      "Epoch 299: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0890 - accuracy: 0.7124 - val_loss: 1.1130 - val_accuracy: 0.7258\n",
      "Epoch 300/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.9862 - accuracy: 0.7126\n",
      "Epoch 300: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9954 - accuracy: 0.7117 - val_loss: 1.1263 - val_accuracy: 0.7418\n",
      "Epoch 301/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0381 - accuracy: 0.7100\n",
      "Epoch 301: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0536 - accuracy: 0.7101 - val_loss: 1.1515 - val_accuracy: 0.7441\n",
      "Epoch 302/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0474 - accuracy: 0.7194\n",
      "Epoch 302: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0505 - accuracy: 0.7187 - val_loss: 1.1240 - val_accuracy: 0.7436\n",
      "Epoch 303/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.9927 - accuracy: 0.7123\n",
      "Epoch 303: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9858 - accuracy: 0.7127 - val_loss: 1.1511 - val_accuracy: 0.7470\n",
      "Epoch 304/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.7139\n",
      "Epoch 304: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0268 - accuracy: 0.7140 - val_loss: 1.1276 - val_accuracy: 0.7459\n",
      "Epoch 305/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.7079\n",
      "Epoch 305: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0161 - accuracy: 0.7071 - val_loss: 1.1124 - val_accuracy: 0.7447\n",
      "Epoch 306/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0473 - accuracy: 0.7155\n",
      "Epoch 306: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0497 - accuracy: 0.7140 - val_loss: 1.0966 - val_accuracy: 0.7327\n",
      "Epoch 307/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.1678 - accuracy: 0.7075\n",
      "Epoch 307: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1763 - accuracy: 0.7087 - val_loss: 1.0790 - val_accuracy: 0.7533\n",
      "Epoch 308/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0049 - accuracy: 0.7137\n",
      "Epoch 308: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0093 - accuracy: 0.7128 - val_loss: 1.1005 - val_accuracy: 0.7378\n",
      "Epoch 309/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0239 - accuracy: 0.7186\n",
      "Epoch 309: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0248 - accuracy: 0.7178 - val_loss: 1.0593 - val_accuracy: 0.7390\n",
      "Epoch 310/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0656 - accuracy: 0.7185\n",
      "Epoch 310: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0650 - accuracy: 0.7178 - val_loss: 1.0798 - val_accuracy: 0.7418\n",
      "Epoch 311/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0192 - accuracy: 0.7125\n",
      "Epoch 311: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0134 - accuracy: 0.7128 - val_loss: 1.0857 - val_accuracy: 0.7499\n",
      "Epoch 312/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0315 - accuracy: 0.7104\n",
      "Epoch 312: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0315 - accuracy: 0.7104 - val_loss: 1.1411 - val_accuracy: 0.7436\n",
      "Epoch 313/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0815 - accuracy: 0.7134\n",
      "Epoch 313: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0955 - accuracy: 0.7130 - val_loss: 1.1543 - val_accuracy: 0.7596\n",
      "Epoch 314/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.7117\n",
      "Epoch 314: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0547 - accuracy: 0.7117 - val_loss: 1.1392 - val_accuracy: 0.7355\n",
      "Epoch 315/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0325 - accuracy: 0.7093\n",
      "Epoch 315: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0417 - accuracy: 0.7089 - val_loss: 1.1017 - val_accuracy: 0.7413\n",
      "Epoch 316/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0981 - accuracy: 0.7067\n",
      "Epoch 316: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0949 - accuracy: 0.7065 - val_loss: 1.1039 - val_accuracy: 0.7453\n",
      "Epoch 317/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0126 - accuracy: 0.7168\n",
      "Epoch 317: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.0077 - accuracy: 0.7178 - val_loss: 1.1457 - val_accuracy: 0.7487\n",
      "Epoch 318/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.1137 - accuracy: 0.7095\n",
      "Epoch 318: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0963 - accuracy: 0.7120 - val_loss: 1.1305 - val_accuracy: 0.7459\n",
      "Epoch 319/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0103 - accuracy: 0.7142\n",
      "Epoch 319: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0075 - accuracy: 0.7125 - val_loss: 1.1022 - val_accuracy: 0.7424\n",
      "Epoch 320/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0415 - accuracy: 0.7107\n",
      "Epoch 320: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0410 - accuracy: 0.7107 - val_loss: 1.1523 - val_accuracy: 0.7441\n",
      "Epoch 321/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0342 - accuracy: 0.7080\n",
      "Epoch 321: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0333 - accuracy: 0.7081 - val_loss: 1.1039 - val_accuracy: 0.7441\n",
      "Epoch 322/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0910 - accuracy: 0.7081\n",
      "Epoch 322: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0902 - accuracy: 0.7084 - val_loss: 1.1384 - val_accuracy: 0.7459\n",
      "Epoch 323/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0712 - accuracy: 0.7130\n",
      "Epoch 323: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0620 - accuracy: 0.7125 - val_loss: 1.1085 - val_accuracy: 0.7521\n",
      "Epoch 324/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0204 - accuracy: 0.7089\n",
      "Epoch 324: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0176 - accuracy: 0.7105 - val_loss: 1.1695 - val_accuracy: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0345 - accuracy: 0.7121\n",
      "Epoch 325: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0347 - accuracy: 0.7145 - val_loss: 1.1295 - val_accuracy: 0.7533\n",
      "Epoch 326/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0123 - accuracy: 0.7174\n",
      "Epoch 326: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0140 - accuracy: 0.7184 - val_loss: 1.1675 - val_accuracy: 0.7333\n",
      "Epoch 327/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0128 - accuracy: 0.7262\n",
      "Epoch 327: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0068 - accuracy: 0.7253 - val_loss: 1.0961 - val_accuracy: 0.7418\n",
      "Epoch 328/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.9417 - accuracy: 0.7205\n",
      "Epoch 328: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9511 - accuracy: 0.7188 - val_loss: 1.1316 - val_accuracy: 0.7384\n",
      "Epoch 329/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0469 - accuracy: 0.7079\n",
      "Epoch 329: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0526 - accuracy: 0.7074 - val_loss: 1.1006 - val_accuracy: 0.7441\n",
      "Epoch 330/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0766 - accuracy: 0.7145\n",
      "Epoch 330: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0766 - accuracy: 0.7144 - val_loss: 1.1490 - val_accuracy: 0.7487\n",
      "Epoch 331/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0218 - accuracy: 0.7215\n",
      "Epoch 331: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0218 - accuracy: 0.7215 - val_loss: 1.1224 - val_accuracy: 0.7579\n",
      "Epoch 332/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0176 - accuracy: 0.7218\n",
      "Epoch 332: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0258 - accuracy: 0.7205 - val_loss: 1.1646 - val_accuracy: 0.7539\n",
      "Epoch 333/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0644 - accuracy: 0.7066\n",
      "Epoch 333: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0581 - accuracy: 0.7077 - val_loss: 1.1103 - val_accuracy: 0.7396\n",
      "Epoch 334/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.7152\n",
      "Epoch 334: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0206 - accuracy: 0.7154 - val_loss: 1.1200 - val_accuracy: 0.7453\n",
      "Epoch 335/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0866 - accuracy: 0.7174\n",
      "Epoch 335: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.0863 - accuracy: 0.7177 - val_loss: 1.0955 - val_accuracy: 0.7510\n",
      "Epoch 336/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0529 - accuracy: 0.7097\n",
      "Epoch 336: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 2s 16ms/step - loss: 1.0531 - accuracy: 0.7094 - val_loss: 1.1153 - val_accuracy: 0.7590\n",
      "Epoch 337/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0461 - accuracy: 0.7130\n",
      "Epoch 337: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.0552 - accuracy: 0.7127 - val_loss: 1.1032 - val_accuracy: 0.7378\n",
      "Epoch 338/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0124 - accuracy: 0.7147\n",
      "Epoch 338: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0222 - accuracy: 0.7142 - val_loss: 1.0888 - val_accuracy: 0.7487\n",
      "Epoch 339/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.1142 - accuracy: 0.7201\n",
      "Epoch 339: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.1142 - accuracy: 0.7201 - val_loss: 1.0873 - val_accuracy: 0.7481\n",
      "Epoch 340/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0130 - accuracy: 0.7168\n",
      "Epoch 340: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.0141 - accuracy: 0.7160 - val_loss: 1.1687 - val_accuracy: 0.7293\n",
      "Epoch 341/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0031 - accuracy: 0.7081\n",
      "Epoch 341: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 1.0180 - accuracy: 0.7081 - val_loss: 1.1457 - val_accuracy: 0.7470\n",
      "Epoch 342/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.9771 - accuracy: 0.7183\n",
      "Epoch 342: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9761 - accuracy: 0.7187 - val_loss: 1.1181 - val_accuracy: 0.7470\n",
      "Epoch 343/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.1157 - accuracy: 0.7101\n",
      "Epoch 343: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.1117 - accuracy: 0.7108 - val_loss: 1.1396 - val_accuracy: 0.7459\n",
      "Epoch 344/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.9792 - accuracy: 0.7160\n",
      "Epoch 344: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9817 - accuracy: 0.7167 - val_loss: 1.1415 - val_accuracy: 0.7424\n",
      "Epoch 345/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.7106\n",
      "Epoch 345: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.0476 - accuracy: 0.7104 - val_loss: 1.1204 - val_accuracy: 0.7344\n",
      "Epoch 346/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0729 - accuracy: 0.7113\n",
      "Epoch 346: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 1.0805 - accuracy: 0.7095 - val_loss: 1.0798 - val_accuracy: 0.7510\n",
      "Epoch 347/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0829 - accuracy: 0.7020\n",
      "Epoch 347: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 1.0836 - accuracy: 0.7029 - val_loss: 1.0802 - val_accuracy: 0.7487\n",
      "Epoch 348/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0232 - accuracy: 0.7100\n",
      "Epoch 348: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 1.0339 - accuracy: 0.7081 - val_loss: 1.0932 - val_accuracy: 0.7401\n",
      "Epoch 349/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0970 - accuracy: 0.7080\n",
      "Epoch 349: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0946 - accuracy: 0.7088 - val_loss: 1.1236 - val_accuracy: 0.7607\n",
      "Epoch 350/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0839 - accuracy: 0.7144\n",
      "Epoch 350: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0771 - accuracy: 0.7144 - val_loss: 1.1197 - val_accuracy: 0.7521\n",
      "Epoch 351/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0721 - accuracy: 0.7188\n",
      "Epoch 351: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0721 - accuracy: 0.7188 - val_loss: 1.1492 - val_accuracy: 0.7562\n",
      "Epoch 352/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0543 - accuracy: 0.7066\n",
      "Epoch 352: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0485 - accuracy: 0.7081 - val_loss: 1.1045 - val_accuracy: 0.7510\n",
      "Epoch 353/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0692 - accuracy: 0.7175\n",
      "Epoch 353: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0733 - accuracy: 0.7187 - val_loss: 1.1560 - val_accuracy: 0.7396\n",
      "Epoch 354/375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/110 [=========================>....] - ETA: 0s - loss: 1.0690 - accuracy: 0.7116\n",
      "Epoch 354: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0771 - accuracy: 0.7118 - val_loss: 1.1425 - val_accuracy: 0.7407\n",
      "Epoch 355/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 1.0251 - accuracy: 0.7141\n",
      "Epoch 355: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0337 - accuracy: 0.7132 - val_loss: 1.1372 - val_accuracy: 0.7384\n",
      "Epoch 356/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0610 - accuracy: 0.7215\n",
      "Epoch 356: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0562 - accuracy: 0.7204 - val_loss: 1.1202 - val_accuracy: 0.7602\n",
      "Epoch 357/375\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 1.0411 - accuracy: 0.7179\n",
      "Epoch 357: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0311 - accuracy: 0.7167 - val_loss: 1.1307 - val_accuracy: 0.7562\n",
      "Epoch 358/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.0432 - accuracy: 0.7148\n",
      "Epoch 358: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0408 - accuracy: 0.7141 - val_loss: 1.1926 - val_accuracy: 0.7493\n",
      "Epoch 359/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0475 - accuracy: 0.7053\n",
      "Epoch 359: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0532 - accuracy: 0.7051 - val_loss: 1.0952 - val_accuracy: 0.7447\n",
      "Epoch 360/375\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 1.0224 - accuracy: 0.7145\n",
      "Epoch 360: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0285 - accuracy: 0.7157 - val_loss: 1.0526 - val_accuracy: 0.7607\n",
      "Epoch 361/375\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 1.0522 - accuracy: 0.7090\n",
      "Epoch 361: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0541 - accuracy: 0.7088 - val_loss: 1.1243 - val_accuracy: 0.7418\n",
      "Epoch 362/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0348 - accuracy: 0.7130\n",
      "Epoch 362: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0484 - accuracy: 0.7132 - val_loss: 1.0821 - val_accuracy: 0.7579\n",
      "Epoch 363/375\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 1.0481 - accuracy: 0.7165\n",
      "Epoch 363: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0505 - accuracy: 0.7142 - val_loss: 1.1750 - val_accuracy: 0.7476\n",
      "Epoch 364/375\n",
      "109/110 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.7112\n",
      "Epoch 364: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0281 - accuracy: 0.7115 - val_loss: 1.0906 - val_accuracy: 0.7579\n",
      "Epoch 365/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0495 - accuracy: 0.7093\n",
      "Epoch 365: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0457 - accuracy: 0.7095 - val_loss: 1.0965 - val_accuracy: 0.7470\n",
      "Epoch 366/375\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.9887 - accuracy: 0.7195\n",
      "Epoch 366: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 1.0027 - accuracy: 0.7195 - val_loss: 1.0845 - val_accuracy: 0.7521\n",
      "Epoch 367/375\n",
      "108/110 [============================>.] - ETA: 0s - loss: 1.0366 - accuracy: 0.7148\n",
      "Epoch 367: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0383 - accuracy: 0.7145 - val_loss: 1.0658 - val_accuracy: 0.7602\n",
      "Epoch 368/375\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 1.0224 - accuracy: 0.7169\n",
      "Epoch 368: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0161 - accuracy: 0.7171 - val_loss: 1.0765 - val_accuracy: 0.7407\n",
      "Epoch 369/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.0155 - accuracy: 0.7216\n",
      "Epoch 369: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0215 - accuracy: 0.7204 - val_loss: 1.0649 - val_accuracy: 0.7556\n",
      "Epoch 370/375\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 1.0719 - accuracy: 0.7169\n",
      "Epoch 370: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.0605 - accuracy: 0.7168 - val_loss: 1.0966 - val_accuracy: 0.7378\n",
      "Epoch 371/375\n",
      "107/110 [============================>.] - ETA: 0s - loss: 1.1492 - accuracy: 0.7116\n",
      "Epoch 371: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.1456 - accuracy: 0.7114 - val_loss: 1.0817 - val_accuracy: 0.7556\n",
      "Epoch 372/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0234 - accuracy: 0.7052\n",
      "Epoch 372: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0234 - accuracy: 0.7052 - val_loss: 1.0750 - val_accuracy: 0.7504\n",
      "Epoch 373/375\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0108 - accuracy: 0.7214\n",
      "Epoch 373: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1.0108 - accuracy: 0.7214 - val_loss: 1.1043 - val_accuracy: 0.7464\n",
      "Epoch 374/375\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 1.1479 - accuracy: 0.7161\n",
      "Epoch 374: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1.1360 - accuracy: 0.7164 - val_loss: 1.1695 - val_accuracy: 0.7527\n",
      "Epoch 375/375\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 1.0540 - accuracy: 0.7178\n",
      "Epoch 375: val_loss did not improve from 1.01560\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 1.0516 - accuracy: 0.7135 - val_loss: 1.1604 - val_accuracy: 0.7413\n",
      "Training completed in time:  0:04:00.465344\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 375\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='Saved model files/audio_classification2.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1c8af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412707209587097\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b122648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 150ms/step\n",
      "Predicted class: 9\n"
     ]
    }
   ],
   "source": [
    "filename = \"UrbanSound8K/UrbanSound8K/dog_bark.wav\"\n",
    "prediction_feature = features_extractor(filename)\n",
    "prediction_feature = prediction_feature.reshape(1, -1)\n",
    "class_probabilities = model.predict(prediction_feature)\n",
    "predicted_class = np.argmax(class_probabilities)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f88fc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f387c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-466.17957  ,    1.0950246,  -34.01389  ,   35.33935  ,\n",
       "        -14.88148  ,  -19.12843  ,   -0.581684 ,  -16.130579 ,\n",
       "        -21.339075 ,    7.673634 ,  -29.16449  ,  -18.950253 ,\n",
       "         -2.9579992,   -8.162329 ,  -15.153101 ,   -6.604805 ,\n",
       "         -7.5685983,    9.340646 ,   14.4331   ,   21.934181 ,\n",
       "         20.861397 ,    1.3340123,  -19.228804 ,   -4.630231 ,\n",
       "         -1.0564743,    3.215267 ,   -6.984281 ,  -16.414577 ,\n",
       "        -10.0286455,   13.009956 ,    0.5334608,  -23.843391 ,\n",
       "        -15.267321 ,    9.245734 ,   10.367627 ,   -0.5832011,\n",
       "         -1.2624055,   17.700016 ,   13.847463 ,   -5.1862826],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "568b27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 4, 4, ..., 1, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = np.argmax(model.predict(X_test), axis=1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26a67f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-234.667       165.31572     -44.493927    -19.632557    -37.42034\n",
      "  -27.384491    -17.713993     16.158552    -25.928234      4.182973\n",
      "    2.575766      5.502858      1.6123058    10.968642      4.634508\n",
      "   12.642901     11.025836      3.138549      1.1661669     4.394597\n",
      "    3.4053905    -2.079561      1.621202      8.355597     -4.522607\n",
      "    0.6237473     4.690442     -1.4363034     0.34019774    2.7678218\n",
      "   -1.7496414    -5.4708095    -2.487572     -2.0299692    -1.1838927\n",
      "    3.64202       8.069101     -1.3552366     1.1179284    -1.9288133 ]\n",
      "[[-234.667       165.31572     -44.493927    -19.632557    -37.42034\n",
      "   -27.384491    -17.713993     16.158552    -25.928234      4.182973\n",
      "     2.575766      5.502858      1.6123058    10.968642      4.634508\n",
      "    12.642901     11.025836      3.138549      1.1661669     4.394597\n",
      "     3.4053905    -2.079561      1.621202      8.355597     -4.522607\n",
      "     0.6237473     4.690442     -1.4363034     0.34019774    2.7678218\n",
      "    -1.7496414    -5.4708095    -2.487572     -2.0299692    -1.1838927\n",
      "     3.64202       8.069101     -1.3552366     1.1179284    -1.9288133 ]]\n",
      "(1, 40)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "filename = \"UrbanSound8K/UrbanSound8K/dog_bark.wav\"\n",
    "\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast')\n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1, -1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "\n",
    "predicted_probabilities = model.predict(mfccs_scaled_features)\n",
    "predicted_label = np.argmax(predicted_probabilities, axis=1)\n",
    "print(predicted_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
